{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70612b7",
   "metadata": {
    "papermill": {
     "duration": 0.007108,
     "end_time": "2025-02-06T05:51:18.226553",
     "exception": false,
     "start_time": "2025-02-06T05:51:18.219445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CZii 2-Stage model\n",
    "The first two parts (YOLO & UNET forked from YukiZ [here](https://www.kaggle.com/code/hideyukizushi/czii-yolo11-unet3d-monai-lb-707)\n",
    "\n",
    "My final strategy:  \n",
    "- Use a blend of two object detectors (YOLOv11 & UNET) with lowered thresholds to provide regions of interest with high recall but low precision.\n",
    "- Use the centroid of each ROI, and crop 32x32x32 arrays around it.\n",
    "- Reshape that array by selecting 12 slices in the z direction, and arranging the arrays 3x2x2\n",
    "- Now I have a 3x64 'image' that I feed to a mixture of 7 TIMM image backbones.\n",
    "- 3 x TTA, so final prediction was a mean of 21 slightly different scores.\n",
    "- A prediction is made, if any 2x detectors agreed with my models above a threshold of 0.4\n",
    "- Also a prediction is made if 1x detector agreed with my model above a threshold of 0.55\n",
    "- Models were trained on Sythetic and Real Data, cropped to 40x40x40 around labelled centroids\n",
    "\n",
    "This wasn't my original goal. I wanted to use Hessian blob detection or some similarly classical approach, then rely only on my own models for classification.  But at this point my models appear to be relatively weak on their own. Ignoring the class predictions of the detectors, my LB scores were only in the 60s.  In my own training I get CV scores with F4 metric in the mid 90s.  So I think the issue must be in generalising to the new data.\n",
    "\n",
    "# **《《《 YOLO 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94107a8b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-06T05:51:18.239841Z",
     "iopub.status.busy": "2025-02-06T05:51:18.239587Z",
     "iopub.status.idle": "2025-02-06T05:52:42.291623Z",
     "shell.execute_reply": "2025-02-06T05:52:42.290933Z"
    },
    "papermill": {
     "duration": 84.060122,
     "end_time": "2025-02-06T05:52:42.293002",
     "exception": false,
     "start_time": "2025-02-06T05:51:18.232880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n",
    "!pip install --no-index --find-links=./packages ultralytics\n",
    "!rm -rf ./packages\n",
    "try:\n",
    "    import zarr\n",
    "except: \n",
    "    !cp -r '/kaggle/input/hengck-czii-cryo-et-01/wheel_file' '/kaggle/working/'\n",
    "    !pip install /kaggle/working/wheel_file/asciitree-0.3.3/asciitree-0.3.3\n",
    "    !pip install --no-index --find-links=/kaggle/working/wheel_file zarr\n",
    "    !pip install --no-index --find-links=/kaggle/working/wheel_file connected-components-3d\n",
    "from typing import List, Tuple, Union\n",
    "deps_path = '/kaggle/input/czii-cryoet-dependencies'\n",
    "! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/hengck-czii-cryo-et-01')\n",
    "from czii_helper import *\n",
    "from dataset import *\n",
    "from model2 import *\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f352acf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:52:42.306881Z",
     "iopub.status.busy": "2025-02-06T05:52:42.306632Z",
     "iopub.status.idle": "2025-02-06T05:53:01.198119Z",
     "shell.execute_reply": "2025-02-06T05:53:01.197152Z"
    },
    "papermill": {
     "duration": 18.900124,
     "end_time": "2025-02-06T05:53:01.199876",
     "exception": false,
     "start_time": "2025-02-06T05:52:42.299752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import zarr\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import defaultdict\n",
    "import lightning.pytorch as pl\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import cc3d\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import Compose, EnsureType\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99358508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:53:01.214262Z",
     "iopub.status.busy": "2025-02-06T05:53:01.213597Z",
     "iopub.status.idle": "2025-02-06T05:53:34.683802Z",
     "shell.execute_reply": "2025-02-06T05:53:34.682830Z"
    },
    "papermill": {
     "duration": 33.478876,
     "end_time": "2025-02-06T05:53:34.685563",
     "exception": false,
     "start_time": "2025-02-06T05:53:01.206687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "#Imports for reclassification stage\n",
    "\n",
    "#Standard Python\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "#Machine Learning\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "import timm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,  EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "#MONAI\n",
    "from monai.transforms import (\n",
    "    Lambda,\n",
    "    Compose, \n",
    "    OneOf,\n",
    "    RandRotate90,\n",
    "    RandFlip,\n",
    "    RandSpatialCrop,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa4ec17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:53:34.699749Z",
     "iopub.status.busy": "2025-02-06T05:53:34.699502Z",
     "iopub.status.idle": "2025-02-06T05:53:36.112080Z",
     "shell.execute_reply": "2025-02-06T05:53:36.111359Z"
    },
    "papermill": {
     "duration": 1.421562,
     "end_time": "2025-02-06T05:53:36.113774",
     "exception": false,
     "start_time": "2025-02-06T05:53:34.692212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/kaggle/input/czii-yolo-l-trained-with-synthetic-data/best_synthetic.pt'\n",
    "model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce431d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:53:36.127946Z",
     "iopub.status.busy": "2025-02-06T05:53:36.127719Z",
     "iopub.status.idle": "2025-02-06T05:53:36.166889Z",
     "shell.execute_reply": "2025-02-06T05:53:36.166318Z"
    },
    "papermill": {
     "duration": 0.047405,
     "end_time": "2025-02-06T05:53:36.168199",
     "exception": false,
     "start_time": "2025-02-06T05:53:36.120794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs_path = '/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/*'\n",
    "runs = sorted(glob.glob(runs_path))\n",
    "runs = [os.path.basename(run) for run in runs]\n",
    "sp = len(runs)//2\n",
    "runs1 = runs[:sp]\n",
    "runs1[:5]\n",
    "\n",
    "#add by @minfuka\n",
    "runs2 = runs[sp:]\n",
    "runs2[:5]\n",
    "\n",
    "#add by @minfuka\n",
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c6a483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:53:36.181962Z",
     "iopub.status.busy": "2025-02-06T05:53:36.181745Z",
     "iopub.status.idle": "2025-02-06T05:53:36.185894Z",
     "shell.execute_reply": "2025-02-06T05:53:36.185297Z"
    },
    "papermill": {
     "duration": 0.012293,
     "end_time": "2025-02-06T05:53:36.187074",
     "exception": false,
     "start_time": "2025-02-06T05:53:36.174781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "particle_names = [\n",
    "    'apo-ferritin',\n",
    "    'beta-amylase',\n",
    "    'beta-galactosidase',\n",
    "    'ribosome',\n",
    "    'thyroglobulin',\n",
    "    'virus-like-particle'\n",
    "]\n",
    "\n",
    "particle_to_index = {\n",
    "    'apo-ferritin': 0,\n",
    "    'beta-amylase': 1,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 3,\n",
    "    'thyroglobulin': 4,\n",
    "    'virus-like-particle': 5\n",
    "}\n",
    "\n",
    "index_to_particle = {index: name for name, index in particle_to_index.items()}\n",
    "\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 60,\n",
    "    'beta-amylase': 65,\n",
    "    'beta-galactosidase': 90,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942ef2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:53:36.201161Z",
     "iopub.status.busy": "2025-02-06T05:53:36.200962Z",
     "iopub.status.idle": "2025-02-06T05:53:36.218392Z",
     "shell.execute_reply": "2025-02-06T05:53:36.217780Z"
    },
    "papermill": {
     "duration": 0.025723,
     "end_time": "2025-02-06T05:53:36.219634",
     "exception": false,
     "start_time": "2025-02-06T05:53:36.193911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add by @sesasj\n",
    "class UnionFind:\n",
    "    def __init__(self, size):\n",
    "        self.parent = np.arange(size)\n",
    "        self.rank = np.zeros(size, dtype=int)\n",
    "\n",
    "    def find(self, u):\n",
    "        if self.parent[u] != u:\n",
    "            self.parent[u] = self.find(self.parent[u])  \n",
    "        return self.parent[u]\n",
    "\n",
    "    def union(self, u, v):\n",
    "        u_root = self.find(u)\n",
    "        v_root = self.find(v)\n",
    "        if u_root == v_root:\n",
    "            return\n",
    "            \n",
    "        if self.rank[u_root] < self.rank[v_root]:\n",
    "            self.parent[u_root] = v_root\n",
    "        else:\n",
    "            self.parent[v_root] = u_root\n",
    "            if self.rank[u_root] == self.rank[v_root]:\n",
    "                self.rank[u_root] += 1\n",
    "\n",
    "class PredictionAggregator:\n",
    "    def __init__(self, first_conf=0.2, conf_coef=0.75):\n",
    "        self.first_conf = first_conf\n",
    "        self.conf_coef = conf_coef\n",
    "        ##############################################################################################################################\n",
    "        ##############################################################################################################################\n",
    "        #self.particle_confs = np.array([0.5, 0.0, 0.2, 0.5, 0.2, 0.5])\n",
    "        self.particle_confs = np.array([0.3, 0.3, 0.3, 0.3, 0.3, 0.3])\n",
    "        ##############################################################################################################################\n",
    "        ##############################################################################################################################\n",
    "        \n",
    "    def convert_to_8bit(self, volume):\n",
    "        lower, upper = np.percentile(volume, (0.5, 99.5))\n",
    "        clipped = np.clip(volume, lower, upper)\n",
    "        scaled = ((clipped - lower) / (upper - lower + 1e-12) * 255).astype(np.uint8)\n",
    "        return scaled\n",
    "\n",
    "    def make_predictions(self, run_id, model, device_no):\n",
    "        volume_path = f'/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/{run_id}/VoxelSpacing10.000/denoised.zarr'\n",
    "        volume = zarr.open(volume_path, mode='r')[0]\n",
    "        volume_8bit = self.convert_to_8bit(volume)\n",
    "        num_slices = volume_8bit.shape[0]\n",
    "\n",
    "        detections = {\n",
    "            'particle_type': [],\n",
    "            'confidence': [],\n",
    "            'x': [],\n",
    "            'y': [],\n",
    "            'z': []\n",
    "        }\n",
    "\n",
    "        for slice_idx in range(num_slices):\n",
    "            \n",
    "            img = volume_8bit[slice_idx]\n",
    "            input_image = cv2.resize(np.stack([img]*3, axis=-1), (640, 640))\n",
    "\n",
    "            results = model.predict(\n",
    "                input_image,\n",
    "                save=False,\n",
    "                imgsz=640,\n",
    "                conf=self.first_conf,\n",
    "                device=device_no,\n",
    "                batch=1,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is None:\n",
    "                    continue\n",
    "                cls = boxes.cls.cpu().numpy().astype(int)\n",
    "                conf = boxes.conf.cpu().numpy()\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "                xc = ((xyxy[:, 0] + xyxy[:, 2]) / 2.0) * 10 * (63/64) # 63/64 because of the resize\n",
    "                yc = ((xyxy[:, 1] + xyxy[:, 3]) / 2.0) * 10 * (63/64)\n",
    "                zc = np.full(xc.shape, slice_idx * 10 + 5)\n",
    "\n",
    "                particle_types = [index_to_particle[c] for c in cls]\n",
    "\n",
    "                detections['particle_type'].extend(particle_types)\n",
    "                detections['confidence'].extend(conf)\n",
    "                detections['x'].extend(xc)\n",
    "                detections['y'].extend(yc)\n",
    "                detections['z'].extend(zc)\n",
    "\n",
    "        if not detections['particle_type']:\n",
    "            return pd.DataFrame()  \n",
    "\n",
    "        particle_types = np.array(detections['particle_type'])\n",
    "        confidences = np.array(detections['confidence'])\n",
    "        xs = np.array(detections['x'])\n",
    "        ys = np.array(detections['y'])\n",
    "        zs = np.array(detections['z'])\n",
    "\n",
    "        aggregated_data = []\n",
    "\n",
    "        for idx, particle in enumerate(particle_names):\n",
    "            if particle == 'beta-amylase':\n",
    "                continue \n",
    "\n",
    "            mask = (particle_types == particle)\n",
    "            if not np.any(mask):\n",
    "                continue  \n",
    "                \n",
    "            particle_confidences = confidences[mask]\n",
    "            particle_xs = xs[mask]\n",
    "            particle_ys = ys[mask]\n",
    "            particle_zs = zs[mask]\n",
    "            # -------------modified by @sersasj ------------------------\n",
    "            coords = np.vstack((particle_xs, particle_ys, particle_zs)).T\n",
    "\n",
    "           \n",
    "            z_distance = 30 # How many slices can you \"jump\" to aggregate predictions 10 = 1, 20 = 2...\n",
    "            xy_distance = 20 # xy_tol_p2 in original code by ITK8191\n",
    "            \n",
    "            max_distance = math.sqrt(z_distance**2 + xy_distance**2)\n",
    "            tree = cKDTree(coords)            \n",
    "            pairs = tree.query_pairs(r=max_distance, p=2)\n",
    "\n",
    "            \n",
    "            uf = UnionFind(len(coords))\n",
    "            \n",
    "            coords_xy = coords[:, :2]\n",
    "            coords_z = coords[:, 2]\n",
    "            for u, v in pairs:\n",
    "                z_diff = abs(coords_z[u] - coords_z[v])\n",
    "                if z_diff > z_distance:\n",
    "                    continue  \n",
    "\n",
    "                xy_diff = np.linalg.norm(coords_xy[u] - coords_xy[v])\n",
    "                if xy_diff > xy_distance:\n",
    "                    continue  \n",
    "\n",
    "                uf.union(u, v)\n",
    "\n",
    "            roots = np.array([uf.find(i) for i in range(len(coords))])\n",
    "            unique_roots, inverse_indices, counts = np.unique(roots, return_inverse=True, return_counts=True)\n",
    "            conf_sums = np.bincount(inverse_indices, weights=particle_confidences)\n",
    "            \n",
    "            aggregated_confidences = conf_sums / (counts ** self.conf_coef)\n",
    "            cluster_per_particle = [4,1,2,9,4,8]\n",
    "            valid_clusters = (counts >= cluster_per_particle[idx]) & (aggregated_confidences > self.particle_confs[idx])\n",
    "\n",
    "            if not np.any(valid_clusters):\n",
    "                continue  \n",
    "\n",
    "            cluster_ids = unique_roots[valid_clusters]\n",
    "\n",
    "            centers_x = np.bincount(inverse_indices, weights=particle_xs) / counts\n",
    "            centers_y = np.bincount(inverse_indices, weights=particle_ys) / counts\n",
    "            centers_z = np.bincount(inverse_indices, weights=particle_zs) / counts\n",
    "\n",
    "            centers_x = centers_x[valid_clusters]\n",
    "            centers_y = centers_y[valid_clusters]\n",
    "            centers_z = centers_z[valid_clusters]\n",
    "\n",
    "            aggregated_df = pd.DataFrame({\n",
    "                'experiment': [run_id] * len(centers_x),\n",
    "                'particle_type': [particle] * len(centers_x),\n",
    "                'x': centers_x,\n",
    "                'y': centers_y,\n",
    "                'z': centers_z\n",
    "            })\n",
    "\n",
    "            aggregated_data.append(aggregated_df)\n",
    "\n",
    "        if aggregated_data:\n",
    "            return pd.concat(aggregated_data, axis=0)\n",
    "        else:\n",
    "            return pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d614c90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:53:36.233535Z",
     "iopub.status.busy": "2025-02-06T05:53:36.233238Z",
     "iopub.status.idle": "2025-02-06T05:54:03.929159Z",
     "shell.execute_reply": "2025-02-06T05:54:03.928230Z"
    },
    "papermill": {
     "duration": 27.704503,
     "end_time": "2025-02-06T05:54:03.930668",
     "exception": false,
     "start_time": "2025-02-06T05:53:36.226165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.67s/it]\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated total prediction time for 500 runs: 4614.8793 seconds\n"
     ]
    }
   ],
   "source": [
    "# instance main class\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "#aggregator = PredictionAggregator(first_conf=0.22,  conf_coef=0.39) #Update\n",
    "aggregator = PredictionAggregator(first_conf=0.17,  conf_coef=0.39) #Olly's Update\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "aggregated_results = []\n",
    "#add by @minfuka\n",
    "from concurrent.futures import ProcessPoolExecutor #add by @minfuka\n",
    "\n",
    "#add by @minfuka\n",
    "def inference(runs, model, device_no):\n",
    "    subs = []\n",
    "    for r in tqdm(runs, total=len(runs)):\n",
    "        df = aggregator.make_predictions(r, model, device_no)\n",
    "        subs.append(df)\n",
    "    \n",
    "    return subs\n",
    "start_time = time.time()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    results = list(executor.map(inference, (runs1, runs2), (model, model), (\"0\", \"1\")))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "estimated_total_time = (end_time - start_time) / len(runs) * 500  \n",
    "print(f'estimated total prediction time for 500 runs: {estimated_total_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a863b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:54:03.946894Z",
     "iopub.status.busy": "2025-02-06T05:54:03.946523Z",
     "iopub.status.idle": "2025-02-06T05:54:03.954087Z",
     "shell.execute_reply": "2025-02-06T05:54:03.953251Z"
    },
    "papermill": {
     "duration": 0.017035,
     "end_time": "2025-02-06T05:54:03.955488",
     "exception": false,
     "start_time": "2025-02-06T05:54:03.938453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change by @minfuka\n",
    "submission0 = pd.concat(results[0])\n",
    "submission1 = pd.concat(results[1])\n",
    "submission_ = pd.concat([submission0, submission1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c268c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:54:03.970464Z",
     "iopub.status.busy": "2025-02-06T05:54:03.970184Z",
     "iopub.status.idle": "2025-02-06T05:54:03.978973Z",
     "shell.execute_reply": "2025-02-06T05:54:03.978293Z"
    },
    "papermill": {
     "duration": 0.017771,
     "end_time": "2025-02-06T05:54:03.980428",
     "exception": false,
     "start_time": "2025-02-06T05:54:03.962657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_.insert(0, 'id', range(len(submission_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983e9e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:54:03.995718Z",
     "iopub.status.busy": "2025-02-06T05:54:03.995484Z",
     "iopub.status.idle": "2025-02-06T05:54:04.011330Z",
     "shell.execute_reply": "2025-02-06T05:54:04.010474Z"
    },
    "papermill": {
     "duration": 0.025041,
     "end_time": "2025-02-06T05:54:04.012707",
     "exception": false,
     "start_time": "2025-02-06T05:54:03.987666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>991</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>4512.710061</td>\n",
       "      <td>5146.912497</td>\n",
       "      <td>1180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>5098.582835</td>\n",
       "      <td>4126.212465</td>\n",
       "      <td>1035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>1451.382610</td>\n",
       "      <td>4807.155391</td>\n",
       "      <td>1125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3708.760895</td>\n",
       "      <td>5591.139954</td>\n",
       "      <td>1232.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>2411.612640</td>\n",
       "      <td>5142.742615</td>\n",
       "      <td>1530.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id experiment        particle_type            x            y       z\n",
       "991  991     TS_6_4  virus-like-particle  4512.710061  5146.912497  1180.0\n",
       "992  992     TS_6_4  virus-like-particle  5098.582835  4126.212465  1035.0\n",
       "993  993     TS_6_4  virus-like-particle  1451.382610  4807.155391  1125.0\n",
       "994  994     TS_6_4  virus-like-particle  3708.760895  5591.139954  1232.5\n",
       "995  995     TS_6_4  virus-like-particle  2411.612640  5142.742615  1530.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04299167",
   "metadata": {
    "papermill": {
     "duration": 0.007871,
     "end_time": "2025-02-06T05:54:04.028722",
     "exception": false,
     "start_time": "2025-02-06T05:54:04.020851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **《《《 Unet3D(Monai) 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "537deac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:54:04.046235Z",
     "iopub.status.busy": "2025-02-06T05:54:04.045960Z",
     "iopub.status.idle": "2025-02-06T05:55:02.807622Z",
     "shell.execute_reply": "2025-02-06T05:55:02.806869Z"
    },
    "papermill": {
     "duration": 58.772131,
     "end_time": "2025-02-06T05:55:02.809098",
     "exception": false,
     "start_time": "2025-02-06T05:54:04.036967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-4cb51b5d90db>:142: DeprecationWarning: config_type not found in config file, defaulting to filesystem\n",
      "  root = copick.from_file(copick_test_config_path)\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 174.08it/s]\n",
      "100%|██████████| 98/98 [00:12<00:00,  7.93it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 142.88it/s]\n",
      "100%|██████████| 98/98 [00:12<00:00,  8.13it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 134.97it/s]\n",
      "100%|██████████| 98/98 [00:12<00:00,  8.10it/s]\n"
     ]
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 7,\n",
    "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n",
    "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n",
    "        num_res_units: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.hparams.spatial_dims,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            out_channels=self.hparams.out_channels,\n",
    "            channels=self.hparams.channels,\n",
    "            strides=self.hparams.strides,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "channels = (48, 64, 80, 80)\n",
    "strides_pattern = (2, 2, 1)\n",
    "num_res_units = 1\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "    \n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "    \n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "    \n",
    "    patch_size = patches[0].shape[0]\n",
    "    \n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch\n",
    "        \n",
    "    \n",
    "    return reconstructed\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "        \n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "    \n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "    \n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "    \n",
    "    return positions\n",
    "import pandas as pd\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset-exp-name\"\n",
    "import json\n",
    "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
    "\n",
    "with open(copick_config_path) as f:\n",
    "    copick_config = json.load(f)\n",
    "\n",
    "copick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n",
    "\n",
    "copick_test_config_path = 'copick_test.config'\n",
    "\n",
    "with open(copick_test_config_path, 'w') as outfile:\n",
    "    json.dump(copick_config, outfile)\n",
    "import copick\n",
    "\n",
    "root = copick.from_file(copick_test_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 11\n",
    "tomo_type = \"denoised\"\n",
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
    "])\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "BLOB_THRESHOLD = 140\n",
    "CERTAINTY_THRESHOLD = 0.035\n",
    "#BLOB_THRESHOLD = 255\n",
    "#CERTAINTY_THRESHOLD = 0.05\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "def load_models(model_paths):\n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        channels = (48, 64, 80, 80)\n",
    "        strides_pattern = (2, 2, 1)       \n",
    "        num_res_units = 1\n",
    "        learning_rate = 1e-3\n",
    "        num_epochs = 100\n",
    "        model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units)\n",
    "        \n",
    "        weights =torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(weights)\n",
    "        model.to('cuda')\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    '/kaggle/input/cziials-a-230-unet/UNet-Model-val_metric0.450.ckpt',\n",
    "]\n",
    "\n",
    "\n",
    "models = load_models(model_paths)\n",
    "def ensemble_prediction_tta(models, input_tensor, threshold=0.5):\n",
    "    probs_list = []\n",
    "    data_copy0 = input_tensor.clone()\n",
    "    data_copy0=torch.flip(data_copy0, dims=[2])\n",
    "    data_copy1 = input_tensor.clone()\n",
    "    data_copy1=torch.flip(data_copy1, dims=[3])\n",
    "    data_copy2 = input_tensor.clone()\n",
    "    data_copy2=torch.flip(data_copy2, dims=[4])\n",
    "    data_copy3 = input_tensor.clone()\n",
    "    data_copy3 = data_copy3.rot90(1, dims=[3, 4])\n",
    "    with torch.no_grad():\n",
    "        model_output0 = model(input_tensor)\n",
    "        model_output1 = model(data_copy0)\n",
    "        model_output1=torch.flip(model_output1, dims=[2])\n",
    "        model_output2 = model(data_copy1)\n",
    "        model_output2=torch.flip(model_output2, dims=[3])\n",
    "        model_output3 = model(data_copy2)\n",
    "        model_output3=torch.flip(model_output3, dims=[4])\n",
    "        probs0 = torch.softmax(model_output0[0], dim=0)\n",
    "        probs1 = torch.softmax(model_output1[0], dim=0)\n",
    "        probs2 = torch.softmax(model_output2[0], dim=0)\n",
    "        probs3 = torch.softmax(model_output3[0], dim=0)\n",
    "        probs_list.append(probs0)\n",
    "        probs_list.append(probs1)\n",
    "        probs_list.append(probs2)\n",
    "        probs_list.append(probs3)\n",
    "    avg_probs = torch.mean(torch.stack(probs_list), dim=0)\n",
    "    thresh_probs = avg_probs > threshold\n",
    "    _, max_classes = thresh_probs.max(dim=0)\n",
    "    return max_classes\n",
    "sub=[]\n",
    "for model in models:\n",
    "    with torch.no_grad():\n",
    "        location_df = []\n",
    "        for run in root.runs:\n",
    "            tomo = run.get_voxel_spacing(10)\n",
    "            tomo = tomo.get_tomogram(tomo_type).numpy()\n",
    "            tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], 96)\n",
    "            tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "            tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n",
    "            pred_masks = []\n",
    "            for i in tqdm(range(len(tomo_ds))):\n",
    "                input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
    "                max_classes = ensemble_prediction_tta(models, input_tensor, threshold=CERTAINTY_THRESHOLD)\n",
    "                pred_masks.append(max_classes.cpu().numpy())\n",
    "            reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
    "            location = {}\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 转换单位\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                location[id_to_name[c]] = xyz\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_df.append(df)\n",
    "        location_df = pd.concat(location_df)\n",
    "        location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bd03f3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:02.852955Z",
     "iopub.status.busy": "2025-02-06T05:55:02.852301Z",
     "iopub.status.idle": "2025-02-06T05:55:02.862104Z",
     "shell.execute_reply": "2025-02-06T05:55:02.861373Z"
    },
    "papermill": {
     "duration": 0.032587,
     "end_time": "2025-02-06T05:55:02.863453",
     "exception": false,
     "start_time": "2025-02-06T05:55:02.830866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>884</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>126.022125</td>\n",
       "      <td>5623.885237</td>\n",
       "      <td>1053.390669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>885</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>1455.693728</td>\n",
       "      <td>4808.580410</td>\n",
       "      <td>1114.245736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>886</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>4516.127768</td>\n",
       "      <td>5150.053215</td>\n",
       "      <td>1164.027166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>887</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3109.298117</td>\n",
       "      <td>782.166881</td>\n",
       "      <td>1243.345624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>888</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>2393.727994</td>\n",
       "      <td>5138.904552</td>\n",
       "      <td>1515.507083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id experiment        particle_type            x            y  \\\n",
       "323  884     TS_6_4  virus-like-particle   126.022125  5623.885237   \n",
       "324  885     TS_6_4  virus-like-particle  1455.693728  4808.580410   \n",
       "325  886     TS_6_4  virus-like-particle  4516.127768  5150.053215   \n",
       "326  887     TS_6_4  virus-like-particle  3109.298117   782.166881   \n",
       "327  888     TS_6_4  virus-like-particle  2393.727994  5138.904552   \n",
       "\n",
       "               z  \n",
       "323  1053.390669  \n",
       "324  1114.245736  \n",
       "325  1164.027166  \n",
       "326  1243.345624  \n",
       "327  1515.507083  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe4b49",
   "metadata": {
    "papermill": {
     "duration": 0.020422,
     "end_time": "2025-02-06T05:55:02.905517",
     "exception": false,
     "start_time": "2025-02-06T05:55:02.885095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **《《《 Blend Detection Models 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba821a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:02.947450Z",
     "iopub.status.busy": "2025-02-06T05:55:02.947190Z",
     "iopub.status.idle": "2025-02-06T05:55:04.196681Z",
     "shell.execute_reply": "2025-02-06T05:55:04.195690Z"
    },
    "papermill": {
     "duration": 1.27243,
     "end_time": "2025-02-06T05:55:04.198528",
     "exception": false,
     "start_time": "2025-02-06T05:55:02.926098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_['weight'] = 1\n",
    "submission_['model_type'] = 'yolo'   \n",
    "location_df['weight'] = 1\n",
    "location_df['model_type'] = 'unet'\n",
    "\n",
    "df = pd.concat([submission_,location_df], ignore_index=True)\n",
    "\n",
    "particle_names = ['apo-ferritin', 'beta-amylase', 'beta-galactosidase', 'ribosome', 'thyroglobulin', 'virus-like-particle']\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 60,\n",
    "    'beta-amylase': 65,\n",
    "    'beta-galactosidase': 90,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}\n",
    "\n",
    "final = []\n",
    "for pidx, p in enumerate(particle_names):\n",
    "    '''The goal here is to keep all predictions, \n",
    "    but assign a weight of 1 to single-model predictions, 2 to 2-model predictions'''\n",
    "    pdf = df[df['particle_type'] == p].reset_index(drop=True)\n",
    "    p_rad = particle_radius[p]\n",
    "    \n",
    "    grouped = pdf.groupby(['experiment'])\n",
    "\n",
    "    \n",
    "    for exp, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        coords = group[['x', 'y', 'z']].values\n",
    "        db = DBSCAN(eps=p_rad, min_samples=2, metric='euclidean').fit(coords)\n",
    "        labels = db.labels_\n",
    "        group['cluster'] = labels\n",
    "        \n",
    "        for cluster_id in np.unique(labels):\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "            \n",
    "            cluster_points = group[group['cluster'] == cluster_id]\n",
    "            unique_model_types = cluster_points['model_type'].nunique()\n",
    "            \n",
    "            if len(cluster_points) > 1:\n",
    "                if unique_model_types >= 2:\n",
    "                    group.loc[group['cluster'] == cluster_id, 'weight'] = 2  #so we still need one more model\n",
    "                \n",
    "                avg_x = cluster_points['x'].mean()\n",
    "                avg_y = cluster_points['y'].mean()\n",
    "                avg_z = cluster_points['z'].mean()\n",
    "                \n",
    "                group.loc[group['cluster'] == cluster_id, ['x', 'y', 'z']] = avg_x, avg_y, avg_z\n",
    "        \n",
    "        # Check for deduplication effect\n",
    "        group = group.drop_duplicates(subset=['x', 'y', 'z'])\n",
    "        final.append(group)\n",
    "\n",
    "df_save = pd.concat(final, ignore_index=True)\n",
    "df_save = df_save.drop(columns=['cluster'])\n",
    "df_roi = df_save.sort_values(by=['experiment', 'particle_type']).reset_index(drop=True)\n",
    "df_roi['id'] = np.arange(0, len(df_roi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed5e8d",
   "metadata": {
    "papermill": {
     "duration": 0.020619,
     "end_time": "2025-02-06T05:55:04.242210",
     "exception": false,
     "start_time": "2025-02-06T05:55:04.221591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **《《《 Reclassify ROIs 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa7796da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:04.285083Z",
     "iopub.status.busy": "2025-02-06T05:55:04.284810Z",
     "iopub.status.idle": "2025-02-06T05:55:04.294847Z",
     "shell.execute_reply": "2025-02-06T05:55:04.294017Z"
    },
    "papermill": {
     "duration": 0.033087,
     "end_time": "2025-02-06T05:55:04.296212",
     "exception": false,
     "start_time": "2025-02-06T05:55:04.263125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>weight</th>\n",
       "      <th>model_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>1295</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>6003.722744</td>\n",
       "      <td>2290.265819</td>\n",
       "      <td>712.909502</td>\n",
       "      <td>1</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1296</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>6112.628157</td>\n",
       "      <td>2456.344802</td>\n",
       "      <td>696.269087</td>\n",
       "      <td>1</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>1297</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>4420.367819</td>\n",
       "      <td>96.296153</td>\n",
       "      <td>909.155157</td>\n",
       "      <td>1</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1298</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3836.956875</td>\n",
       "      <td>3585.742541</td>\n",
       "      <td>1005.818223</td>\n",
       "      <td>1</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1299</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3109.298117</td>\n",
       "      <td>782.166881</td>\n",
       "      <td>1243.345624</td>\n",
       "      <td>1</td>\n",
       "      <td>unet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id experiment        particle_type            x            y  \\\n",
       "1295  1295     TS_6_4  virus-like-particle  6003.722744  2290.265819   \n",
       "1296  1296     TS_6_4  virus-like-particle  6112.628157  2456.344802   \n",
       "1297  1297     TS_6_4  virus-like-particle  4420.367819    96.296153   \n",
       "1298  1298     TS_6_4  virus-like-particle  3836.956875  3585.742541   \n",
       "1299  1299     TS_6_4  virus-like-particle  3109.298117   782.166881   \n",
       "\n",
       "                z  weight model_type  \n",
       "1295   712.909502       1       unet  \n",
       "1296   696.269087       1       unet  \n",
       "1297   909.155157       1       unet  \n",
       "1298  1005.818223       1       unet  \n",
       "1299  1243.345624       1       unet  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roi.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb1f1679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:04.339976Z",
     "iopub.status.busy": "2025-02-06T05:55:04.339718Z",
     "iopub.status.idle": "2025-02-06T05:55:04.346780Z",
     "shell.execute_reply": "2025-02-06T05:55:04.345873Z"
    },
    "papermill": {
     "duration": 0.030928,
     "end_time": "2025-02-06T05:55:04.348075",
     "exception": false,
     "start_time": "2025-02-06T05:55:04.317147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferConfig:\n",
    "    '''Wrapper class for inference & model configuration'''\n",
    "    def __init__(self):\n",
    "        self.EXTRA_CORES = None #None will result in the max cpu count used.\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.ROI_SIZE = 32\n",
    "        self.ZARR_SCALE = 10.012444\n",
    "        self.CLASS_NAMES = ['apo-ferritin',\n",
    "                            'beta-amylase',\n",
    "                            'beta-galactosidase',\n",
    "                            'empty',\n",
    "                            'ribosome',\n",
    "                            'thyroglobulin',\n",
    "                            'virus-like-particle']\n",
    "        self.MODELS_OVER_THRESHOLD = 3\n",
    "\n",
    "class ImageConfig:\n",
    "    '''Wrapper class for image processing parameters'''\n",
    "    INPUT_MEAN = [ 0.485, 0.456, 0.406 ] #values from ImageNet.\n",
    "    INPUT_STD = [ 0.229, 0.224, 0.225 ] #values from ImageNet.\n",
    "\n",
    "#Just commenting out most of the models so no need to make them all public, but allow the notebook to run\n",
    "MODELS = [ #{'backbone': 'eca_nfnet_l0.ra2_in1k',  #mean CV: 0.955,  mean - sd = 0.936\n",
    "           #  'weights': '/kaggle/input/czii_exp_24_ecanfnet_2sl/pytorch/default/1/Run_01_best_weights.pt',\n",
    "           #  'activation': 'sigmoid',\n",
    "           #  'slices': (np.arange(8, 32, 2),0),  #(np.arange(10,22), 0), (np.arange(8, 32, 2),0)\n",
    "           #  'thresholds': {'apo-ferritin': 0.36,\n",
    "           #       'beta-amylase': 0.36,\n",
    "           #       'beta-galactosidase': 0.36,\n",
    "           #       'empty': 0.6,\n",
    "           #       'ribosome': 0.36,\n",
    "           #       'thyroglobulin' : 0.36,\n",
    "           #       'virus-like-particle': 0.36},\n",
    "           #  'tta': [0,1,2]\n",
    "           # },\n",
    "\n",
    "           # {'backbone': 'efficientvit_b3.r288_in1k',    #mean CV:  0.946,  mean-sd: 0.938\n",
    "           #  'weights': '/kaggle/input/czii_exp_22_efficientvit_2sl/pytorch/default/1/Run_01_best_weights.pt',\n",
    "           #  'activation': 'sigmoid',\n",
    "           #  'slices': (np.arange(8, 32, 2),0),  #(np.arange(10,22), 0), (np.arange(8, 32, 2),0)\n",
    "           #  'thresholds': {'apo-ferritin': 0.45,\n",
    "           #       'beta-amylase': 0.45,\n",
    "           #       'beta-galactosidase': 0.45,\n",
    "           #       'empty': 0.6,\n",
    "           #       'ribosome': 0.45,\n",
    "           #       'thyroglobulin' : 0.45,\n",
    "           #       'virus-like-particle': 0.45},\n",
    "           #  'tta': [2,3,4]\n",
    "           # },\n",
    "          \n",
    "            {'backbone': 'mobilevit_xs.cvnets_in1k',   \n",
    "             'weights': '/kaggle/input/czii_exp_21_mobilevitxs_2sl/pytorch/default/1/Run_01_best_weights.pt',  #7 classes, mean 0.961,  mean-sd  0.947\n",
    "            # 'weights': '/kaggle/input/czii_exp_29_mobilevitxs_6_2sl/pytorch/default/1/Run_01_best_weights.pt', # 6 class, mean 0.964,  0.947\n",
    "             'activation': 'sigmoid',\n",
    "             'slices': (np.arange(8, 32, 2),0),  #(np.arange(10,22), 0), (np.arange(8, 32, 2),0)\n",
    "             'thresholds': {'apo-ferritin': 0.36,\n",
    "                  'beta-amylase': 0.36,\n",
    "                  'beta-galactosidase': 0.36,\n",
    "                  'empty': 0.6,  #remove for 6-class models\n",
    "                  'ribosome': 0.36,\n",
    "                  'thyroglobulin' : 0.36,\n",
    "                  'virus-like-particle': 0.36},\n",
    "             'tta': [4,5,0]\n",
    "            },\n",
    "\n",
    "\n",
    "            #{'backbone': 'mobilevitv2_175.cvnets_in22k_ft_in1k_384',     #0.944,  0.933\n",
    "            # 'weights': '/kaggle/input/czii_exp_26_mobilevit2_2sl/pytorch/default/1/Run_01_best_weights.pt',\n",
    "            # 'activation': 'sigmoid',\n",
    "            # 'slices': (np.arange(8, 32, 2),0),  #(np.arange(10,22), 0), (np.arange(8, 32, 2),0)\n",
    "            # 'thresholds': {'apo-ferritin': 0.45,\n",
    "            #      'beta-amylase': 0.45,\n",
    "            #      'beta-galactosidase': 0.45,\n",
    "            #      'empty': 0.6,    #remove for 6-class models\n",
    "            #      'ribosome': 0.45,\n",
    "            #      'thyroglobulin' : 0.45,\n",
    "            #      'virus-like-particle': 0.45},\n",
    "            # 'tta': [5,0,1]\n",
    "            #},\n",
    "\n",
    "\n",
    "            ### Older ones, before I started randomising more layers for pre-train\n",
    "\n",
    "            #Tried dropping this, but the score went backwards.  Have a go at imroving it.\n",
    "            #{'backbone': 'mobilevit_s.cvnets_in1k', #Exp_10   MobileViT  Central 12 slices  \n",
    "            # 'weights': '/kaggle/input/czii_exp_10_run_03_mobilevit/pytorch/default/1/Run_03_best_weights.pt',   #.948, 0.928 (7 classes)\n",
    "             #'weights': '/kaggle/input/czii_exp_30_mobilevits_6_1sl/pytorch/default/1/Run_01_best_weights.pt',   #.953, 0.934 (6 classes)\n",
    "            # 'activation': 'sigmoid',\n",
    "            # 'slices': (np.arange(10,22), 0), #(np.arange(10,22), 0), (np.arange(8, 32, 2),0)\n",
    "            # 'thresholds': {'apo-ferritin': 0.45,\n",
    "            #      'beta-amylase': 0.45,\n",
    "            #      'beta-galactosidase': 0.45,\n",
    "            #      'empty': 0.6,\n",
    "            #      'ribosome': 0.45,\n",
    "            #      'thyroglobulin' : 0.45,\n",
    "            #      'virus-like-particle': 0.45},\n",
    "            # 'tta': [0,1,2]\n",
    "            #},\n",
    "\n",
    "            #{'backbone': 'mobilevit_xs.cvnets_in1k',  #Exp_14  mobilevit_xs   12 central slices   # 0.958,  0.943\n",
    "            # 'weights':'/kaggle/input/czii_exp_14_run_02/pytorch/default/1/Run_02_best_weights.pt',\n",
    "            # 'activation': 'sigmoid',\n",
    "            # 'slices': (np.arange(10,22), 0), #(np.arange(10,22), 0), (np.arange(8, 32, 2),0)\n",
    "            # 'thresholds': {'apo-ferritin': 0.45,\n",
    "             #     'beta-amylase': 0.45,\n",
    "           #      'beta-galactosidase': 0.45,\n",
    "            #      'empty': 0.6,\n",
    "            #      'ribosome': 0.45,\n",
    "            #      'thyroglobulin' : 0.45,\n",
    "            #      'virus-like-particle': 0.45},\n",
    "            # 'tta': [1,2,3]\n",
    "            #},\n",
    "\n",
    "            #{'backbone': 'mobilevit_s.cvnets_in1k',  #Exp_09   MobileViT Every second slice    #0.954  0.944\n",
    "            #             'weights': '/kaggle/input/czii_exp_00_run_02_mobilevit/pytorch/default/1/Run_02_best_weights.pt',\n",
    "            # 'activation': 'sigmoid',\n",
    "            # 'slices': (np.arange(8, 32, 2),0), #(np.arange(10,22), 0),\n",
    "            # 'thresholds': {'apo-ferritin': 0.45,\n",
    "            #      'beta-amylase': 0.45,\n",
    "            #      'beta-galactosidase': 0.45,\n",
    "            #      'empty': 0.6,\n",
    "            #      'ribosome': 0.45,\n",
    "            #      'thyroglobulin' : 0.45,\n",
    "            #      'virus-like-particle': 0.45},\n",
    "            # 'tta': [2,3,4]\n",
    "            #},\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebaa8e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:04.398559Z",
     "iopub.status.busy": "2025-02-06T05:55:04.398207Z",
     "iopub.status.idle": "2025-02-06T05:55:04.433761Z",
     "shell.execute_reply": "2025-02-06T05:55:04.432869Z"
    },
    "papermill": {
     "duration": 0.062575,
     "end_time": "2025-02-06T05:55:04.435258",
     "exception": false,
     "start_time": "2025-02-06T05:55:04.372683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(kwargs, gpu_idx):\n",
    "    backbone = kwargs['backbone']\n",
    "    weights_path = kwargs['weights']   \n",
    "    #device = torch.device(f\"cuda:{kwargs['gpu']}\")\n",
    "    ckpt = torch.load(weights_path)#[\"model_state_dict\"]\n",
    "    model = TomoModel(model_name=backbone, num_classes = len(kwargs['thresholds']))\n",
    "    model.load_state_dict(ckpt) #[\"model_state_dict\"]\n",
    "    model.eval()\n",
    "    model = model.to(f'cuda:{gpu_idx}')\n",
    "    #model = torch.nn.DataParallel(model, device_ids = [0,1]).to(torch.device('cuda'))\n",
    "    return model\n",
    "\n",
    "\n",
    "class Augmentation():\n",
    "    '''Vol Augmentation to work with 3d tensors Outputting (z,y,x)\n",
    "       Img Augmentation to work with numpy arrays, initial shape (20,20,3) or similar\n",
    "       Outputing (3,16,16) as a PyTorch Tensor (C,H,W)'''\n",
    "\n",
    "    def _slice_one_dim(self, array, slice_pattern):\n",
    "        \"\"\"\n",
    "        Apply a slice or indices dynamically to the first or second dimension of a 3D array.\n",
    "        slice_pattern is a tuple (the pattern (array, list of slice), the dimension (0 or 1)\n",
    "        \"\"\"\n",
    "        pattern= slice_pattern[0]\n",
    "        dim = slice_pattern[1]\n",
    "        if isinstance(pattern, (slice, list, np.ndarray)):\n",
    "            if dim==0:\n",
    "                return array[pattern]\n",
    "            else: \n",
    "                return array[:, pattern]\n",
    "        else:\n",
    "            raise ValueError(\"Slice pattern must be a slice, list, or numpy array.\")\n",
    "    \n",
    "    def __init__(self, mean, std, height, width, layer_pattern):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        self.vol_ttas = [\n",
    "                         Compose([Lambda(func = lambda x: self._slice_one_dim(x, layer_pattern)),\n",
    "                                RandSpatialCrop(roi_size=(32, 32), random_center=False, random_size=False)\n",
    "                                ]),\n",
    "                        Compose([\n",
    "                                RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                Lambda(func = lambda x: self._slice_one_dim(x, layer_pattern)),\n",
    "                                RandSpatialCrop(roi_size=(32, 32), random_center=False, random_size=False)\n",
    "                                ]),\n",
    "                        Compose([\n",
    "                                RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                Lambda(func = lambda x: self._slice_one_dim(x, layer_pattern)),\n",
    "                                RandSpatialCrop(roi_size=(32, 32), random_center=False, random_size=False)\n",
    "                                ]),\n",
    "                       Compose([\n",
    "                                RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                Lambda(func = lambda x: self._slice_one_dim(x, layer_pattern)),\n",
    "                                RandSpatialCrop(roi_size=(32, 32), random_center=False, random_size=False)\n",
    "                                ]),\n",
    "                        Compose([\n",
    "                                 RandFlip(prob=1, spatial_axis=0),\n",
    "                                 RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                 Lambda(func = lambda x: self._slice_one_dim(x, layer_pattern)),\n",
    "                                 RandSpatialCrop(roi_size=(32, 32), random_center=False, random_size=False)\n",
    "                                ]),\n",
    "                        Compose([\n",
    "                                 RandFlip(prob=1, spatial_axis=1),\n",
    "                                 RandRotate90(prob=1, spatial_axes=[0, 1], max_k=1),\n",
    "                                 Lambda(func = lambda x: self._slice_one_dim(x, layer_pattern)),\n",
    "                                 RandSpatialCrop(roi_size=(32, 32), random_center=False, random_size=False)\n",
    "                                ])\n",
    "                        ]\n",
    "\n",
    "        self.img_val = A.Compose([\n",
    "            A.CenterCrop(height=2*height, width=2*width, p=1),\n",
    "            A.Normalize(mean=self.mean, std=self.std, max_pixel_value=1), \n",
    "            ToTensorV2()]) #Note: ToTensorV2 turns HWC Numpy to CHW Tensor\n",
    "\n",
    "\n",
    "class TomoDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 roi_dict,\n",
    "                 vol_transform=None,\n",
    "                 img_transform=None):\n",
    "        self.df = df\n",
    "        self.vol_transform = vol_transform\n",
    "        self.img_transform = img_transform\n",
    "        self.roi_dict = roi_dict\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def reshape_tomo(self, array):\n",
    "        \"\"\"\n",
    "        Starting with an array shaped (z, y, x) (Slices, Height, Width):\n",
    "        - Crop so the total slices is exactly 12.\n",
    "        - Break into 4 parts along the slice direction.\n",
    "        - Re-assemble the 4 parts so the assembled array is (3, 32, 32).\n",
    "        - Transpose, so it is left as (Height, Width, Channels) to be treated as an image.\n",
    "        \"\"\"\n",
    "\n",
    "        z_slices = array.shape[0]\n",
    "        assert z_slices % 4 == 0, \"The number of slices after cropping must be divisible by 4\"\n",
    "        parts = np.array_split(array, 4, axis=0)\n",
    "        top = np.concatenate(parts[:2], axis=2)  # Concatenate the first two along width\n",
    "        bottom = np.concatenate(parts[2:], axis=2)  # Concatenate the last two along width\n",
    "        assembled_array = np.concatenate([top, bottom], axis=1)  # Concatenate along height\n",
    "        hwc_array = np.transpose(assembled_array, (1, 2, 0))  # Transpose to (y, x, z)\n",
    "        return hwc_array\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        id = row['id']\n",
    "        tomo = self.roi_dict[id]\n",
    "        tomo = self.vol_transform(tomo)\n",
    "        image = self.reshape_tomo(tomo) #Numpy array like an image, of shape (H, W, 3)\n",
    "        min_vals = np.min(image)\n",
    "        max_vals = np.max(image) \n",
    "        image = (image - min_vals) / (max_vals - min_vals)  #Min-Max normalise on [0,1]\n",
    "        image_tensor = self.img_transform(image=image)['image']  #Tensor with channels first (3, H, W)\n",
    "        return image_tensor\n",
    "\n",
    "\n",
    "class BasicHead(nn.Module):\n",
    "    '''Bare bones fc classifier head'''\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(BasicHead, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TomoModel(pl.LightningModule):\n",
    "    '''Pytorch Lightning Model'''\n",
    "    def __init__(self,\n",
    "                model_name = 'resnet18.a1_in1k',\n",
    "                num_classes = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=False)\n",
    "\n",
    "        #varios logic to replace the classifier head depending on model architecture\n",
    "        if model_name.startswith('eca_nfnet') or model_name.startswith('convnext'):\n",
    "            self.in_features = self.backbone.head.fc.in_features\n",
    "            setattr(self.backbone.head, 'fc', BasicHead(self.in_features, self.num_classes))\n",
    "        elif model_name.startswith('efficientvit'): #or model_name.startswith(deit_tiny):\n",
    "            for layer in self.backbone.head.classifier:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    self.in_features = layer.in_features\n",
    "                    break\n",
    "            setattr(self.backbone.head, 'classifier', BasicHead(self.in_features, self.num_classes))\n",
    "        elif model_name.startswith('mobilevit'):\n",
    "            self.in_features = self.backbone.head.fc.in_features\n",
    "            setattr(self.backbone.head, 'fc', BasicHead(self.in_features, self.num_classes))\n",
    "        elif model_name.startswith('deit_tiny'):\n",
    "            self.in_features = self.backbone.head.in_features\n",
    "            setattr(self.backbone.head, 'linear', BasicHead(self.in_features, self.num_classes))\n",
    "        elif hasattr(self.backbone, 'fc'):\n",
    "            self.in_features = self.backbone.fc.in_features\n",
    "            setattr(self.backbone, 'fc', BasicHead(self.in_features, self.num_classes))\n",
    "        elif hasattr(self.backbone, 'classifier'):\n",
    "            self.in_features = self.backbone.classifier.in_features\n",
    "            setattr(self.backbone, 'classifier', BasicHead(self.in_features, self.num_classes))\n",
    "        elif hasattr(self.backbone, 'head') and hasattr(self.backbone.head, 'linear'):\n",
    "            self.in_features = self.backbone.head.linear.in_features\n",
    "            setattr(self.backbone, 'linear', BasicHead(self.in_features, self.num_classes))\n",
    "        else:\n",
    "            raise AttributeError(\"The backbone does not have a 'fc' or 'classifier' or 'head', layer. Update the code to handle the correct layer name.\")\n",
    "\n",
    "    def forward(self, images):\n",
    "        logits = self.backbone(images)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def crop_roi(tomo, row):\n",
    "    slicing_cols = ['object_type','x_start', 'x_end', 'y_start', 'y_end', 'z_start', 'z_end']\n",
    "    records = group[slicing_cols].to_dict(orient='records')\n",
    "    \n",
    "    for idx, row in enumerate(records):\n",
    "        crop = tomo[0][row['z_start']:row['z_end'],\n",
    "                       row['y_start']:row['y_end'],\n",
    "                       row['x_start']:row['x_end']]\n",
    "        type=row['object_type']\n",
    "        np.save(paths.DESTN_FLDRS[type] / f'{sample}_{idx}.npy', crop)\n",
    "\n",
    "\n",
    "def preprocess_rois(df, tomo, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Extract Regions of Interest (ROIs) from a tomograph and store them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing slicing information for ROIs.\n",
    "        folder_pth (Path): Path to the folder containing the tomograph data.\n",
    "        sample_name (str): Name of the sample to process.\n",
    "        size (tuple): Size of the ROI (not directly used but kept for flexibility).\n",
    "        n_jobs (int): Number of parallel jobs. Defaults to -1 (use all available CPUs).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of ROIs keyed by their 'id'.\n",
    "    \"\"\"\n",
    "\n",
    "    slicing_cols = ['id', 'x_start', 'x_end', 'y_start', 'y_end', 'z_start', 'z_end']\n",
    "    records = df[slicing_cols].to_dict(orient='records')\n",
    "\n",
    "    def extract_roi(row):\n",
    "        crop = tomo[row['z_start']:row['z_end'],\n",
    "                    row['y_start']:row['y_end'],\n",
    "                    row['x_start']:row['x_end']]\n",
    "        return row['id'], crop\n",
    "\n",
    "    roi_list = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_roi)(row) for row in records)\n",
    "    roi_dict = {roi_id: crop for roi_id, crop in roi_list}\n",
    "\n",
    "    return roi_dict\n",
    "\n",
    "\n",
    "def open_tomo(experiment, test_folder):\n",
    "        zarr_path = test_folder / experiment / 'VoxelSpacing10.000/denoised.zarr'\n",
    "        volume = zarr.open(zarr_path, mode='r')[0]  # Access the highest resolution tomograph\n",
    "        lower, upper = np.percentile(volume, (0.5, 99.5))\n",
    "        clipped = np.clip(volume, lower, upper)\n",
    "        scaled = ((clipped - lower) / (upper - lower + 1e-12) * 255).astype(np.uint8)\n",
    "        return scaled\n",
    "\n",
    "\n",
    "def predict_with_one_model(experiment, df, model_tuple, tomo, batch_size=32, num_workers=4):\n",
    "    model = model_tuple[0] # adjust this when I have more models\n",
    "    model_args = model_tuple[1]\n",
    "    thresholds = model_args['thresholds']\n",
    "    activation = model_args['activation']\n",
    "    tta_choices = model_args['tta']  #A list of integers\n",
    "    test_folder = Path('/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/')\n",
    "    class_names = {0: \"apo-ferritin\", \n",
    "                   1: \"beta-amylase\",\n",
    "                   2: \"beta-galactosidase\",\n",
    "                   3: \"empty\",\n",
    "                   4: \"ribosome\", \n",
    "                   5: \"thyroglobulin\", \n",
    "                   6: \"virus-like-particle\"}\n",
    "\n",
    "    \n",
    "    rois = preprocess_rois(df, tomo)\n",
    "    \n",
    "    transforms = Augmentation(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                              std  = [ 0.229, 0.224, 0.225 ],\n",
    "                              height = 32,\n",
    "                              width = 32,\n",
    "                              layer_pattern = model_args['slices'])    # mean, std, height, width, layer_pattern\n",
    "\n",
    "    tta_dfs = []\n",
    "    for tta in tta_choices:\n",
    "        ds = TomoDataset(df, rois, transforms.vol_ttas[tta], transforms.img_val)\n",
    "        print(f'inferring experiment {experiment}')\n",
    "        \n",
    "        predictions = []\n",
    "        loader = DataLoader(ds, \n",
    "                        batch_size=batch_size, \n",
    "                        persistent_workers=True,\n",
    "                        num_workers = num_workers, \n",
    "                        shuffle=False,\n",
    "                        drop_last=False\n",
    "                        )\n",
    "        device = model.device\n",
    "        with torch.no_grad():\n",
    "            for images in loader:\n",
    "                images = images.to(device)\n",
    "                logits = model(images)\n",
    "                if activation == 'Softmax':\n",
    "                    probs = F.softmax(logits, dim=1).detach().cpu().numpy()  #batch_size x num_classes\n",
    "                else:\n",
    "                    probs = F.sigmoid(logits).detach().cpu().numpy()  #batch_size x num_classes\n",
    "                predictions.append(probs)\n",
    "        all_predictions = np.vstack(predictions)  # Shape: (total_samples x num_classes)\n",
    "\n",
    "        if all_predictions.shape[1] == 6:\n",
    "            all_predictions = np.insert(all_predictions, 3, 0, axis=1)  #Add 0's for a possible 'empty' column for 6 class models.\n",
    "    \n",
    "        predictions_df = pd.DataFrame(\n",
    "            all_predictions, \n",
    "            columns=[class_names[i] for i in range(all_predictions.shape[1])]\n",
    "        )\n",
    "    \n",
    "        cols_to_keep = ['id', 'experiment',\t'particle_type', 'x', 'y', 'z', 'weight']\n",
    "        _df = df[cols_to_keep].reset_index(drop=True)\n",
    "        tta_df = pd.concat([_df, predictions_df], axis=1)\n",
    "        tta_dfs.append(tta_df)\n",
    "\n",
    "    pred_cols = [val for val in class_names.values()]\n",
    "    stacked_values = np.stack([df[pred_cols].values for df in tta_dfs])\n",
    "    #mean_values = np.mean(stacked_values, axis=0)\n",
    "    combined_values = np.mean(stacked_values, axis=0)\n",
    "    #max_values = np.max(stacked_values, axis=0)\n",
    "    #combined_values = (mean_values + max_values) / 2\n",
    "\n",
    "    df_model = tta_dfs[0].copy()\n",
    "    df_model[pred_cols] = combined_values\n",
    "\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def predict_one_sample(experiment, df, model_tuples, tomo, batch_size=32, num_workers=2):\n",
    "\n",
    "    model_dataframes = []\n",
    "    for model_tuple in model_tuples:\n",
    "        model_df = predict_with_one_model(experiment, df, model_tuple, tomo,batch_size, num_workers)\n",
    "        model_dataframes.append(model_df)\n",
    "\n",
    "    text_columns = ['id', 'x', 'y', 'z', 'experiment', 'particle_type', 'weight']  # Columns to leave unchanged\n",
    "    numeric_columns = model_dataframes[0].columns.difference(text_columns).tolist()  #Columns to sum\n",
    "    #numeric_values = np.sum([df[numeric_columns].values for df in model_dataframes], axis=0)\n",
    "    numeric_values = np.mean([df[numeric_columns].values for df in model_dataframes], axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    numeric_df = pd.DataFrame(numeric_values, columns=numeric_columns)\n",
    "    text_df = model_dataframes[0][text_columns]\n",
    "    result = pd.concat([text_df, numeric_df], axis=1)\n",
    "    result = result[model_dataframes[0].columns]\n",
    "\n",
    "    thresholds = model_tuples[0][1]['thresholds']  #just grabbing the thresholds from the first model in the list\n",
    "    for class_name, threshold in thresholds.items():\n",
    "        original_values = result[class_name]  # Preserve the original values\n",
    "        result[class_name] = (original_values >= threshold).astype(int)  # 0 or 1\n",
    "        result[class_name] += (original_values >= threshold + 0.19).astype(int)  # 0, 1, or 2\n",
    "        #result[class_name] += (original_values >= threshold + 0.64).astype(int)  # 0, 1, 2, or 3  #seems unhelpful\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c06492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:04.484554Z",
     "iopub.status.busy": "2025-02-06T05:55:04.484209Z",
     "iopub.status.idle": "2025-02-06T05:55:04.490899Z",
     "shell.execute_reply": "2025-02-06T05:55:04.490134Z"
    },
    "papermill": {
     "duration": 0.033229,
     "end_time": "2025-02-06T05:55:04.492147",
     "exception": false,
     "start_time": "2025-02-06T05:55:04.458918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_one_half(df, infer_cfg, image_cfg, gpu_idx):\n",
    "    models = [(get_model(args_dict, gpu_idx),args_dict) for args_dict in MODELS] #note, the args_dict may be needed for pre-processing\n",
    "    test_folder = Path('/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/')\n",
    "    preds = {}\n",
    "    \n",
    "    for experiment, sample_df in df.groupby(by='experiment'):\n",
    "        tomo = open_tomo(experiment, test_folder)\n",
    "        preds[experiment] = predict_one_sample(experiment, \n",
    "                                               sample_df, \n",
    "                                               models, \n",
    "                                               tomo, \n",
    "                                               batch_size=32)\n",
    "    return preds\n",
    "\n",
    "def parallel_inference(df, infer_cfg, image_cfg, gpus=[0, 1]):\n",
    "    # Split the DataFrame into two halves\n",
    "    df_0, df_1 = split_dataframe(df)\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "        results = list(executor.map(infer_one_half, \n",
    "                                    [df_0, df_1], \n",
    "                                    [infer_cfg, infer_cfg], \n",
    "                                    [image_cfg, image_cfg], \n",
    "                                    gpus))\n",
    "    \n",
    "    # Merge the results from both halves (use dict union or any other method)\n",
    "    all_preds = all_preds_list[0] | all_preds_list[1]\n",
    "    \n",
    "    return all_preds\n",
    "\n",
    "def split_dataframe(df):\n",
    "    # Split DataFrame into two nearly equal parts\n",
    "    middle_index = len(df) // 2\n",
    "    df_0 = df.iloc[:middle_index]\n",
    "    df_1 = df.iloc[middle_index:]\n",
    "    return df_0, df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e53873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:04.543142Z",
     "iopub.status.busy": "2025-02-06T05:55:04.542867Z",
     "iopub.status.idle": "2025-02-06T05:55:23.957814Z",
     "shell.execute_reply": "2025-02-06T05:55:23.956782Z"
    },
    "papermill": {
     "duration": 19.442253,
     "end_time": "2025-02-06T05:55:23.959439",
     "exception": false,
     "start_time": "2025-02-06T05:55:04.517186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_5_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_5_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_5_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_69_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_69_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_69_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_6_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_6_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring experiment TS_6_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    infer_cfg = InferConfig()\n",
    "    image_cfg = ImageConfig()\n",
    "    \n",
    "    HALF = infer_cfg.ROI_SIZE//2\n",
    "    roi_df = df_roi.copy()\n",
    "    roi_df['x_v'] = round(roi_df['x'] / infer_cfg.ZARR_SCALE).astype(int).clip(lower=HALF, upper=630-HALF) # 8 - 622\n",
    "    roi_df['y_v'] = round(roi_df['y'] / infer_cfg.ZARR_SCALE).astype(int).clip(lower=HALF, upper=630-HALF) # 8 - 622\n",
    "    roi_df['z_v'] = round(roi_df['z'] / infer_cfg.ZARR_SCALE).astype(int).clip(lower=HALF, upper=184-HALF) # 8 - 176\n",
    "    roi_df['x_start'], roi_df['x_end'] = roi_df['x_v'] - HALF, roi_df['x_v'] + HALF\n",
    "    roi_df['y_start'], roi_df['y_end'] = roi_df['y_v'] - HALF, roi_df['y_v'] + HALF\n",
    "    roi_df['z_start'], roi_df['z_end'] = roi_df['z_v'] - HALF, roi_df['z_v'] + HALF\n",
    "\n",
    "    #all_preds = parallel_inference(roi_df, infer_cfg, image_cfg, [0,1])  #Locks up.  Haven't figured out why.\n",
    "    all_preds = infer_one_half(roi_df, infer_cfg, image_cfg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3540bf24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.004666Z",
     "iopub.status.busy": "2025-02-06T05:55:24.004378Z",
     "iopub.status.idle": "2025-02-06T05:55:24.020074Z",
     "shell.execute_reply": "2025-02-06T05:55:24.019328Z"
    },
    "papermill": {
     "duration": 0.039513,
     "end_time": "2025-02-06T05:55:24.021325",
     "exception": false,
     "start_time": "2025-02-06T05:55:23.981812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>weight</th>\n",
       "      <th>apo-ferritin</th>\n",
       "      <th>beta-amylase</th>\n",
       "      <th>beta-galactosidase</th>\n",
       "      <th>empty</th>\n",
       "      <th>ribosome</th>\n",
       "      <th>thyroglobulin</th>\n",
       "      <th>virus-like-particle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>1295</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>6003.722744</td>\n",
       "      <td>2290.265819</td>\n",
       "      <td>712.909502</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1296</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>6112.628157</td>\n",
       "      <td>2456.344802</td>\n",
       "      <td>696.269087</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>1297</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>4420.367819</td>\n",
       "      <td>96.296153</td>\n",
       "      <td>909.155157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1298</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3836.956875</td>\n",
       "      <td>3585.742541</td>\n",
       "      <td>1005.818223</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1299</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3109.298117</td>\n",
       "      <td>782.166881</td>\n",
       "      <td>1243.345624</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id experiment        particle_type            x            y  \\\n",
       "1295  1295     TS_6_4  virus-like-particle  6003.722744  2290.265819   \n",
       "1296  1296     TS_6_4  virus-like-particle  6112.628157  2456.344802   \n",
       "1297  1297     TS_6_4  virus-like-particle  4420.367819    96.296153   \n",
       "1298  1298     TS_6_4  virus-like-particle  3836.956875  3585.742541   \n",
       "1299  1299     TS_6_4  virus-like-particle  3109.298117   782.166881   \n",
       "\n",
       "                z  weight  apo-ferritin  beta-amylase  beta-galactosidase  \\\n",
       "1295   712.909502       1             0             0                   0   \n",
       "1296   696.269087       1             0             0                   0   \n",
       "1297   909.155157       1             0             0                   0   \n",
       "1298  1005.818223       1             0             0                   0   \n",
       "1299  1243.345624       1             0             0                   0   \n",
       "\n",
       "      empty  ribosome  thyroglobulin  virus-like-particle  \n",
       "1295      0         0              0                    1  \n",
       "1296      0         0              0                    2  \n",
       "1297      0         1              0                    2  \n",
       "1298      0         0              0                    2  \n",
       "1299      0         1              1                    1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat(all_preds.values(), ignore_index=True)\n",
    "results_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed9b6f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.066493Z",
     "iopub.status.busy": "2025-02-06T05:55:24.066227Z",
     "iopub.status.idle": "2025-02-06T05:55:24.075000Z",
     "shell.execute_reply": "2025-02-06T05:55:24.074302Z"
    },
    "papermill": {
     "duration": 0.032646,
     "end_time": "2025-02-06T05:55:24.076249",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.043603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "virus-like-particle\n",
       "0    1223\n",
       "2      51\n",
       "1      26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['virus-like-particle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39290c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.123861Z",
     "iopub.status.busy": "2025-02-06T05:55:24.123651Z",
     "iopub.status.idle": "2025-02-06T05:55:24.128786Z",
     "shell.execute_reply": "2025-02-06T05:55:24.128096Z"
    },
    "papermill": {
     "duration": 0.028708,
     "end_time": "2025-02-06T05:55:24.129963",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.101255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beta-amylase\n",
       "0    982\n",
       "2    167\n",
       "1    151\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta-amylase'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ffbdc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.175796Z",
     "iopub.status.busy": "2025-02-06T05:55:24.175497Z",
     "iopub.status.idle": "2025-02-06T05:55:24.181953Z",
     "shell.execute_reply": "2025-02-06T05:55:24.180998Z"
    },
    "papermill": {
     "duration": 0.031042,
     "end_time": "2025-02-06T05:55:24.183153",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.152111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thyroglobulin\n",
       "0    785\n",
       "2    354\n",
       "1    161\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['thyroglobulin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd8de0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.228279Z",
     "iopub.status.busy": "2025-02-06T05:55:24.228018Z",
     "iopub.status.idle": "2025-02-06T05:55:24.233501Z",
     "shell.execute_reply": "2025-02-06T05:55:24.232621Z"
    },
    "papermill": {
     "duration": 0.029533,
     "end_time": "2025-02-06T05:55:24.234708",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.205175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empty\n",
       "0    1237\n",
       "1      40\n",
       "2      23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['empty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "257bd880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.279922Z",
     "iopub.status.busy": "2025-02-06T05:55:24.279686Z",
     "iopub.status.idle": "2025-02-06T05:55:24.286116Z",
     "shell.execute_reply": "2025-02-06T05:55:24.285262Z"
    },
    "papermill": {
     "duration": 0.030511,
     "end_time": "2025-02-06T05:55:24.287610",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.257099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df['beta-amylase'] = 0\n",
    "results_df = results_df[~(results_df[infer_cfg.CLASS_NAMES] == 0).all(axis=1)]  #remove rows where all reclassify predictions == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d460826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.333725Z",
     "iopub.status.busy": "2025-02-06T05:55:24.333523Z",
     "iopub.status.idle": "2025-02-06T05:55:24.339204Z",
     "shell.execute_reply": "2025-02-06T05:55:24.338237Z"
    },
    "papermill": {
     "duration": 0.029718,
     "end_time": "2025-02-06T05:55:24.340570",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.310852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empty\n",
       "0    1124\n",
       "1      40\n",
       "2      23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['empty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee975cd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.387044Z",
     "iopub.status.busy": "2025-02-06T05:55:24.386820Z",
     "iopub.status.idle": "2025-02-06T05:55:24.393428Z",
     "shell.execute_reply": "2025-02-06T05:55:24.392573Z"
    },
    "papermill": {
     "duration": 0.030314,
     "end_time": "2025-02-06T05:55:24.394628",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.364314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "easy = [\"apo-ferritin\",  \"ribosome\", \"virus-like-particle\"]\n",
    "hard = [\"beta-galactosidase\", \"thyroglobulin\"]\n",
    "certainly_easy = 2\n",
    "mask = results_df[easy].ge(certainly_easy).any(axis=1)\n",
    "results_df.loc[mask, hard] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cce2988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.439819Z",
     "iopub.status.busy": "2025-02-06T05:55:24.439594Z",
     "iopub.status.idle": "2025-02-06T05:55:24.454982Z",
     "shell.execute_reply": "2025-02-06T05:55:24.454316Z"
    },
    "papermill": {
     "duration": 0.039573,
     "end_time": "2025-02-06T05:55:24.456337",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.416764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>weight</th>\n",
       "      <th>apo-ferritin</th>\n",
       "      <th>beta-amylase</th>\n",
       "      <th>beta-galactosidase</th>\n",
       "      <th>empty</th>\n",
       "      <th>ribosome</th>\n",
       "      <th>thyroglobulin</th>\n",
       "      <th>virus-like-particle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>4985.038208</td>\n",
       "      <td>4833.595703</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5873.572518</td>\n",
       "      <td>5136.602123</td>\n",
       "      <td>81.830725</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5712.765924</td>\n",
       "      <td>4998.938474</td>\n",
       "      <td>116.047437</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5747.706355</td>\n",
       "      <td>5110.587367</td>\n",
       "      <td>86.628920</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>5473.405809</td>\n",
       "      <td>1520.826088</td>\n",
       "      <td>84.250002</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id experiment particle_type            x            y           z  weight  \\\n",
       "0   0     TS_5_4  apo-ferritin  4985.038208  4833.595703   20.000000       1   \n",
       "1   1     TS_5_4  apo-ferritin  5873.572518  5136.602123   81.830725       2   \n",
       "2   2     TS_5_4  apo-ferritin  5712.765924  4998.938474  116.047437       2   \n",
       "3   3     TS_5_4  apo-ferritin  5747.706355  5110.587367   86.628920       2   \n",
       "4   4     TS_5_4  apo-ferritin  5473.405809  1520.826088   84.250002       2   \n",
       "\n",
       "   apo-ferritin  beta-amylase  beta-galactosidase  empty  ribosome  \\\n",
       "0             1             0                   0      0         0   \n",
       "1             4             0                   0      0         0   \n",
       "2             4             0                   0      0         0   \n",
       "3             4             0                   0      0         0   \n",
       "4             3             0                   0      0         0   \n",
       "\n",
       "   thyroglobulin  virus-like-particle  \n",
       "0              1                    0  \n",
       "1              0                    0  \n",
       "2              0                    0  \n",
       "3              0                    0  \n",
       "4              0                    0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_names = [\"apo-ferritin\", \n",
    "                \"beta-galactosidase\",\n",
    "                \"ribosome\", \n",
    "                \"thyroglobulin\", \n",
    "                \"virus-like-particle\"]\n",
    "\n",
    "ohe = pd.get_dummies(results_df['particle_type']).reindex(columns=scored_names, fill_value=0)\n",
    "ohe[scored_names] = ohe[scored_names].mul(results_df['weight'], axis=0)\n",
    "results_df[scored_names] += ohe\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18fd72f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.503237Z",
     "iopub.status.busy": "2025-02-06T05:55:24.502980Z",
     "iopub.status.idle": "2025-02-06T05:55:24.590969Z",
     "shell.execute_reply": "2025-02-06T05:55:24.590304Z"
    },
    "papermill": {
     "duration": 0.112324,
     "end_time": "2025-02-06T05:55:24.592228",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.479904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>5873.572518</td>\n",
       "      <td>5136.602123</td>\n",
       "      <td>81.830725</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>5712.765924</td>\n",
       "      <td>4998.938474</td>\n",
       "      <td>116.047437</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>5747.706355</td>\n",
       "      <td>5110.587367</td>\n",
       "      <td>86.628920</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>5473.405809</td>\n",
       "      <td>1520.826088</td>\n",
       "      <td>84.250002</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>5294.213770</td>\n",
       "      <td>4177.939746</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment            x            y           z particle_type  id\n",
       "0     TS_5_4  5873.572518  5136.602123   81.830725  apo-ferritin   0\n",
       "1     TS_5_4  5712.765924  4998.938474  116.047437  apo-ferritin   1\n",
       "2     TS_5_4  5747.706355  5110.587367   86.628920  apo-ferritin   2\n",
       "3     TS_5_4  5473.405809  1520.826088   84.250002  apo-ferritin   3\n",
       "4     TS_5_4  5294.213770  4177.939746  135.000000  apo-ferritin   4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_only_top = False\n",
    "\n",
    "if keep_only_top:\n",
    "    results_df['particle_type'] = results_df.iloc[:, -7:].idxmax(axis=1)\n",
    "else:\n",
    "    cols_to_keep = ['experiment', 'x', 'y', 'z'] #+ infer_cfg.CLASS_NAMES  #For debugging/analysis\n",
    "    positive_rows = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        for column_name in scored_names:\n",
    "            if row[column_name] >= infer_cfg.MODELS_OVER_THRESHOLD:  #At least this many models over threshold to predict a positive.\n",
    "                new_row = {col: row[col] for col in cols_to_keep}\n",
    "                new_row['particle_type'] = column_name\n",
    "                positive_rows.append(new_row)\n",
    "    results_df = pd.DataFrame(positive_rows)\n",
    "\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "results_df['id'] = results_df.index\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdaa1383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.679114Z",
     "iopub.status.busy": "2025-02-06T05:55:24.678802Z",
     "iopub.status.idle": "2025-02-06T05:55:24.689572Z",
     "shell.execute_reply": "2025-02-06T05:55:24.688670Z"
    },
    "papermill": {
     "duration": 0.03533,
     "end_time": "2025-02-06T05:55:24.690779",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.655449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>898</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>2402.670317</td>\n",
       "      <td>5140.823583</td>\n",
       "      <td>1522.753541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>899</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3625.590307</td>\n",
       "      <td>3705.609113</td>\n",
       "      <td>511.765081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>900</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>6112.628157</td>\n",
       "      <td>2456.344802</td>\n",
       "      <td>696.269087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>901</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>4420.367819</td>\n",
       "      <td>96.296153</td>\n",
       "      <td>909.155157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>902</td>\n",
       "      <td>TS_6_4</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3836.956875</td>\n",
       "      <td>3585.742541</td>\n",
       "      <td>1005.818223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id experiment        particle_type            x            y  \\\n",
       "898  898     TS_6_4  virus-like-particle  2402.670317  5140.823583   \n",
       "899  899     TS_6_4  virus-like-particle  3625.590307  3705.609113   \n",
       "900  900     TS_6_4  virus-like-particle  6112.628157  2456.344802   \n",
       "901  901     TS_6_4  virus-like-particle  4420.367819    96.296153   \n",
       "902  902     TS_6_4  virus-like-particle  3836.956875  3585.742541   \n",
       "\n",
       "               z  \n",
       "898  1522.753541  \n",
       "899   511.765081  \n",
       "900   696.269087  \n",
       "901   909.155157  \n",
       "902  1005.818223  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep = ['id','experiment', 'particle_type', 'x', 'y', 'z']\n",
    "results_df = results_df[cols_to_keep]\n",
    "results_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c31bc4bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T05:55:24.737427Z",
     "iopub.status.busy": "2025-02-06T05:55:24.737162Z",
     "iopub.status.idle": "2025-02-06T05:55:24.749002Z",
     "shell.execute_reply": "2025-02-06T05:55:24.748166Z"
    },
    "papermill": {
     "duration": 0.036528,
     "end_time": "2025-02-06T05:55:24.750165",
     "exception": false,
     "start_time": "2025-02-06T05:55:24.713637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10033515,
     "sourceId": 84969,
     "sourceType": "competition"
    },
    {
     "datasetId": 6052780,
     "sourceId": 9862305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6040935,
     "sourceId": 9867543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6465904,
     "sourceId": 10445850,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6484063,
     "sourceId": 10471985,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 206640467,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211097053,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 232761,
     "modelInstanceId": 211064,
     "sourceId": 246980,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 252.015916,
   "end_time": "2025-02-06T05:55:27.731893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-06T05:51:15.715977",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
